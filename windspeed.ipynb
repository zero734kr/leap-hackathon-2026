{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ef2ab1-736e-4422-9d7a-39bb52bcd872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSN_ROOT: s3://leap-pangeo-manual/hackathon-2026/\n",
      "HRRR_PREFIX: s3://leap-pangeo-manual/hackathon-2026/hrrr/\n",
      "WIND_OUT_PREFIX: gs://leap-scratch/renriviera/sfincs_soundview_preproc/forcing/wind_hrrr\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0) Config\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "OSN_ENDPOINT_URL = \"https://nyu1.osn.mghpcc.org\"\n",
    "OSN_BUCKET = \"leap-pangeo-manual\"\n",
    "HACKATHON_PREFIX = \"hackathon-2026/\"\n",
    "OSN_ROOT = f\"s3://{OSN_BUCKET}/{HACKATHON_PREFIX}\"\n",
    "\n",
    "HRRR_PREFIX = f\"{OSN_ROOT}hrrr/\"\n",
    "\n",
    "# Where to write derived outputs (writable scratch)\n",
    "SCRATCH_BUCKET = os.environ.get(\"SCRATCH_BUCKET\", \"gs://leap-scratch/renriviera\")\n",
    "OUT_PREFIX = f\"{SCRATCH_BUCKET}/sfincs_soundview_preproc\"\n",
    "WIND_OUT_PREFIX = f\"{OUT_PREFIX}/forcing/wind_hrrr\"\n",
    "\n",
    "print(\"OSN_ROOT:\", OSN_ROOT)\n",
    "print(\"HRRR_PREFIX:\", HRRR_PREFIX)\n",
    "print(\"WIND_OUT_PREFIX:\", WIND_OUT_PREFIX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b475212b-5eb9-4c34-9f65-4510eb427a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using Dask Gateway cluster\n",
      "<Client: 'tls://10.0.34.165:8786' processes=0 threads=0, memory=0 B>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 22:54:08,130 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1) Dask cluster\n",
    "# ============================================================\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = None\n",
    "\n",
    "try:\n",
    "    from dask_gateway import Gateway\n",
    "    gw = Gateway()\n",
    "    cluster = gw.new_cluster()\n",
    "    cluster.scale(4)  # adjust: 2‚Äì8 workers typical\n",
    "    client = cluster.get_client()\n",
    "    print(\"‚úÖ Using Dask Gateway cluster\")\n",
    "    print(client)\n",
    "except Exception as e:\n",
    "    print(\"Gateway not available (or failed). Falling back to LocalCluster.\")\n",
    "    from dask.distributed import LocalCluster\n",
    "    cluster = LocalCluster(\n",
    "        n_workers=2,\n",
    "        threads_per_worker=2,\n",
    "        memory_limit=\"3GB\",\n",
    "        dashboard_address=\":8787\",\n",
    "    )\n",
    "    client = Client(cluster)\n",
    "    print(\"‚úÖ Using LocalCluster\")\n",
    "    print(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2529b7a2-7d7a-4215-b0bc-a297b7dd58bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to OSN: https://nyu1.osn.mghpcc.org\n",
      "Listing HRRR prefix:\n",
      "['leap-pangeo-manual/hackathon-2026/hrrr/refc', 'leap-pangeo-manual/hackathon-2026/hrrr/temp2m', 'leap-pangeo-manual/hackathon-2026/hrrr/tp', 'leap-pangeo-manual/hackathon-2026/hrrr/u10m', 'leap-pangeo-manual/hackathon-2026/hrrr/v10m']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2) OSN S3 filesystem (anonymous)\n",
    "# ============================================================\n",
    "\n",
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    anon=True,\n",
    "    client_kwargs={\"endpoint_url\": OSN_ENDPOINT_URL},\n",
    ")\n",
    "\n",
    "# quick sanity check\n",
    "print(\"‚úÖ Connected to OSN:\", OSN_ENDPOINT_URL)\n",
    "print(\"Listing HRRR prefix:\")\n",
    "print(fs.ls(HRRR_PREFIX)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a5aac61-6f51-4327-878f-1a3394774b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using YEAR: 2025\n",
      "U10M_STORE: s3://leap-pangeo-manual/hackathon-2026/hrrr/u10m/hrrru10m2025.zarr\n",
      "V10M_STORE: s3://leap-pangeo-manual/hackathon-2026/hrrr/v10m/hrrrv10m2025.zarr\n",
      "‚úÖ Confirmed both 2025 Zarr stores exist on OSN.\n",
      "\n",
      "--- ds_u ---\n",
      "<xarray.Dataset> Size: 59GB\n",
      "Dimensions:              (time: 7800, y: 1059, x: 1799)\n",
      "Coordinates:\n",
      "  * time                 (time) datetime64[ns] 62kB 2025-01-01 ... 2025-11-21...\n",
      "    gribfile_projection  float64 8B ...\n",
      "    heightAboveGround    float64 8B ...\n",
      "    latitude             (y, x) float64 15MB dask.array<chunksize=(1059, 1799), meta=np.ndarray>\n",
      "    longitude            (y, x) float64 15MB dask.array<chunksize=(1059, 1799), meta=np.ndarray>\n",
      "    step                 timedelta64[ns] 8B ...\n",
      "    valid_time           (time) datetime64[ns] 62kB dask.array<chunksize=(24,), meta=np.ndarray>\n",
      "Dimensions without coordinates: y, x\n",
      "Data variables:\n",
      "    u10                  (time, y, x) float32 59GB dask.array<chunksize=(24, 1059, 1799), meta=np.ndarray>\n",
      "Attributes:\n",
      "    GRIB_edition:            2\n",
      "    GRIB_centre:             kwbc\n",
      "    GRIB_centreDescription:  US National Weather Service - NCEP\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             US National Weather Service - NCEP\n",
      "    model:                   hrrr\n",
      "    product:                 sfc\n",
      "    description:             High-Resolution Rapid Refresh - CONUS\n",
      "    search:                  :UGRD:10 m\n",
      "u vars: ['u10']\n",
      "\n",
      "--- ds_v ---\n",
      "<xarray.Dataset> Size: 59GB\n",
      "Dimensions:              (time: 7800, y: 1059, x: 1799)\n",
      "Coordinates:\n",
      "  * time                 (time) datetime64[ns] 62kB 2025-01-01 ... 2025-11-21...\n",
      "    gribfile_projection  float64 8B ...\n",
      "    heightAboveGround    float64 8B ...\n",
      "    latitude             (y, x) float64 15MB dask.array<chunksize=(1059, 1799), meta=np.ndarray>\n",
      "    longitude            (y, x) float64 15MB dask.array<chunksize=(1059, 1799), meta=np.ndarray>\n",
      "    step                 timedelta64[ns] 8B ...\n",
      "    valid_time           (time) datetime64[ns] 62kB dask.array<chunksize=(24,), meta=np.ndarray>\n",
      "Dimensions without coordinates: y, x\n",
      "Data variables:\n",
      "    v10                  (time, y, x) float32 59GB dask.array<chunksize=(24, 1059, 1799), meta=np.ndarray>\n",
      "Attributes:\n",
      "    GRIB_edition:            2\n",
      "    GRIB_centre:             kwbc\n",
      "    GRIB_centreDescription:  US National Weather Service - NCEP\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             US National Weather Service - NCEP\n",
      "    model:                   hrrr\n",
      "    product:                 sfc\n",
      "    description:             High-Resolution Rapid Refresh - CONUS\n",
      "    search:                  :VGRD:10 m\n",
      "v vars: ['v10']\n",
      "\n",
      "--- Time coverage ---\n",
      "u10m time: (np.datetime64('2025-01-01T00:00:00.000000000'), np.datetime64('2025-11-21T23:00:00.000000000'), 7800)\n",
      "v10m time: (np.datetime64('2025-01-01T00:00:00.000000000'), np.datetime64('2025-11-21T23:00:00.000000000'), 7800)\n",
      "‚úÖ u/v time ranges match.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3) Open HRRR u10m/v10m Zarr stores for YEAR=2025 only (OSN S3)\n",
    "# ============================================================\n",
    "\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "\n",
    "YEAR = 2025\n",
    "\n",
    "U10M_STORE = f\"s3://{OSN_BUCKET}/{HACKATHON_PREFIX}hrrr/u10m/hrrru10m{YEAR}.zarr\"\n",
    "V10M_STORE = f\"s3://{OSN_BUCKET}/{HACKATHON_PREFIX}hrrr/v10m/hrrrv10m{YEAR}.zarr\"\n",
    "\n",
    "print(\"‚úÖ Using YEAR:\", YEAR)\n",
    "print(\"U10M_STORE:\", U10M_STORE)\n",
    "print(\"V10M_STORE:\", V10M_STORE)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# IMPORTANT: OSN is S3-compatible, not GCS.\n",
    "# We MUST use an S3 filesystem pointed at the OSN endpoint.\n",
    "# ------------------------------------------------------------\n",
    "fs_s3 = fsspec.filesystem(\n",
    "    \"s3\",\n",
    "    anon=True,\n",
    "    client_kwargs={\"endpoint_url\": OSN_ENDPOINT_URL},\n",
    ")\n",
    "\n",
    "def exists_zarr_s3(store_path: str) -> bool:\n",
    "    # Zarr v2 marker: .zgroup\n",
    "    # Zarr v3 marker: zarr.json\n",
    "    return (\n",
    "        fs_s3.exists(store_path)\n",
    "        or fs_s3.exists(store_path.rstrip(\"/\") + \"/.zgroup\")\n",
    "        or fs_s3.exists(store_path.rstrip(\"/\") + \"/zarr.json\")\n",
    "    )\n",
    "\n",
    "# ---- Existence check\n",
    "if not exists_zarr_s3(U10M_STORE):\n",
    "    raise FileNotFoundError(f\"‚ùå u10m store not found: {U10M_STORE}\")\n",
    "\n",
    "if not exists_zarr_s3(V10M_STORE):\n",
    "    raise FileNotFoundError(f\"‚ùå v10m store not found: {V10M_STORE}\")\n",
    "\n",
    "print(\"‚úÖ Confirmed both 2025 Zarr stores exist on OSN.\")\n",
    "\n",
    "# ---- Open with consolidated fallback\n",
    "def open_zarr_safely_s3(store_path: str):\n",
    "    mapper = fs_s3.get_mapper(store_path)\n",
    "    try:\n",
    "        return xr.open_zarr(mapper, consolidated=True)\n",
    "    except Exception:\n",
    "        return xr.open_zarr(mapper, consolidated=False)\n",
    "\n",
    "ds_u = open_zarr_safely_s3(U10M_STORE)\n",
    "ds_v = open_zarr_safely_s3(V10M_STORE)\n",
    "\n",
    "print(\"\\n--- ds_u ---\")\n",
    "print(ds_u)\n",
    "print(\"u vars:\", list(ds_u.data_vars)[:30])\n",
    "\n",
    "print(\"\\n--- ds_v ---\")\n",
    "print(ds_v)\n",
    "print(\"v vars:\", list(ds_v.data_vars)[:30])\n",
    "\n",
    "# ---- Quick time coverage sanity check\n",
    "def time_range(ds):\n",
    "    if \"time\" not in ds.coords:\n",
    "        return None\n",
    "    t0 = ds[\"time\"].values[0]\n",
    "    t1 = ds[\"time\"].values[-1]\n",
    "    n = ds.sizes.get(\"time\", None)\n",
    "    return t0, t1, n\n",
    "\n",
    "u_t = time_range(ds_u)\n",
    "v_t = time_range(ds_v)\n",
    "\n",
    "print(\"\\n--- Time coverage ---\")\n",
    "print(\"u10m time:\", u_t)\n",
    "print(\"v10m time:\", v_t)\n",
    "\n",
    "if u_t and v_t:\n",
    "    if (u_t[0] != v_t[0]) or (u_t[1] != v_t[1]) or (u_t[2] != v_t[2]):\n",
    "        print(\"‚ö†Ô∏è WARNING: u/v time ranges differ (we will intersect later).\")\n",
    "    else:\n",
    "        print(\"‚úÖ u/v time ranges match.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b67627f6-2cb4-407f-a66c-56ee5a3cc6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected u var: u10\n",
      "Selected v var: v10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/client.py:3371: UserWarning: Sending large graph of size 16.36 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/client.py:3371: UserWarning: Sending large graph of size 16.36 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ wind dataset:\n",
      "<xarray.Dataset> Size: 178GB\n",
      "Dimensions:              (y: 1059, x: 1799, time: 7800)\n",
      "Coordinates:\n",
      "  * time                 (time) datetime64[ns] 62kB 2025-01-01 ... 2025-11-21...\n",
      "    gribfile_projection  float64 8B nan\n",
      "    heightAboveGround    float64 8B 10.0\n",
      "    latitude             (y, x) float64 15MB 21.14 21.15 21.15 ... 47.85 47.84\n",
      "    longitude            (y, x) float64 15MB 237.3 237.3 237.3 ... 299.0 299.1\n",
      "    step                 timedelta64[ns] 8B 00:00:00\n",
      "    valid_time           (time) datetime64[ns] 62kB 2025-01-01 ... 2025-11-21...\n",
      "Dimensions without coordinates: y, x\n",
      "Data variables:\n",
      "    wind10_u             (time, y, x) float32 59GB dask.array<chunksize=(24, 1059, 1799), meta=np.ndarray>\n",
      "    wind10_v             (time, y, x) float32 59GB dask.array<chunksize=(24, 1059, 1799), meta=np.ndarray>\n",
      "    wind10_speed         (time, y, x) float32 59GB dask.array<chunksize=(24, 1059, 1799), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) Extract u10m and v10m variables + standardize names\n",
    "# ============================================================\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "def pick_main_var(ds, prefer_substrings=(\"u10\", \"u10m\", \"v10\", \"v10m\", \"wind\")):\n",
    "    if len(ds.data_vars) == 0:\n",
    "        raise RuntimeError(\"Dataset has no data variables.\")\n",
    "    # Prefer variables that look like wind\n",
    "    for key in ds.data_vars:\n",
    "        lk = key.lower()\n",
    "        if any(s in lk for s in prefer_substrings):\n",
    "            return key\n",
    "    return list(ds.data_vars)[0]\n",
    "\n",
    "u_var = pick_main_var(ds_u, prefer_substrings=(\"u10\", \"u10m\", \"ugrd\", \"wind\"))\n",
    "v_var = pick_main_var(ds_v, prefer_substrings=(\"v10\", \"v10m\", \"vgrd\", \"wind\"))\n",
    "\n",
    "print(\"Selected u var:\", u_var)\n",
    "print(\"Selected v var:\", v_var)\n",
    "\n",
    "u10 = ds_u[u_var].rename(\"wind10_u\")\n",
    "v10 = ds_v[v_var].rename(\"wind10_v\")\n",
    "\n",
    "# Force consistent coordinates (time/x/y)\n",
    "u10, v10 = xr.align(u10, v10, join=\"inner\")\n",
    "\n",
    "wind = xr.Dataset({\"wind10_u\": u10, \"wind10_v\": v10})\n",
    "wind[\"wind10_speed\"] = np.sqrt(wind[\"wind10_u\"]**2 + wind[\"wind10_v\"]**2)\n",
    "\n",
    "print(\"\\n‚úÖ wind dataset:\")\n",
    "print(wind)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72c69c97-d45e-4692-9f3f-f3246590bd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wind dims: FrozenMappingWarningOnValuesAccess({'y': 1059, 'x': 1799, 'time': 7800})\n",
      "Wind coords: ['gribfile_projection', 'heightAboveGround', 'latitude', 'longitude', 'step', 'time', 'valid_time']\n",
      "\n",
      "ROI WGS84 bbox:\n",
      "  lon: (-73.882, -73.842)\n",
      "  lat: (40.807, 40.836)\n",
      "\n",
      "Latitude shape: (1059, 1799) Longitude shape: (1059, 1799)\n",
      "Detected 0..360 longitude convention -> converting to -180..180\n",
      "\n",
      "Mask pixels inside ROI: 2\n",
      "\n",
      "Subset index window:\n",
      "  y: (696, 704) => height: 9\n",
      "  x: (1551, 1560) => width : 10\n",
      "\n",
      "‚úÖ Wind subset done.\n",
      "Subset dims: FrozenMappingWarningOnValuesAccess({'y': 9, 'x': 10, 'time': 7800})\n",
      "‚úÖ Using YEAR: 2025\n",
      "U10M_STORE: s3://leap-pangeo-manual/hackathon-2026/hrrr/u10m/hrrru10m2025.zarr\n",
      "V10M_STORE: s3://leap-pangeo-manual/hackathon-2026/hrrr/v10m/hrrrv10m2025.zarr\n",
      "üßπ deleted stale: ds_u\n",
      "üßπ deleted stale: ds_v\n",
      "üßπ deleted stale: u\n",
      "üßπ deleted stale: v\n",
      "üßπ deleted stale: wind_fews\n"
     ]
    },
    {
     "ename": "GroupNotFoundError",
     "evalue": "No group found in store <fsspec.mapping.FSMap object at 0x7dbc7f36f410> at path ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGroupNotFoundError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mopen_zarr_s3\u001b[39m\u001b[34m(store)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_zarr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/zarr.py:1586\u001b[39m, in \u001b[36mopen_zarr\u001b[39m\u001b[34m(store, group, synchronizer, chunks, decode_cf, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, consolidated, overwrite_encoded_chunks, chunk_store, storage_options, decode_timedelta, use_cftime, zarr_version, zarr_format, use_zarr_fill_value_as_mask, chunked_array_type, from_array_kwargs, create_default_indexes, **kwargs)\u001b[39m\n\u001b[32m   1576\u001b[39m backend_kwargs = {\n\u001b[32m   1577\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msynchronizer\u001b[39m\u001b[33m\"\u001b[39m: synchronizer,\n\u001b[32m   1578\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconsolidated\u001b[39m\u001b[33m\"\u001b[39m: consolidated,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1583\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mzarr_format\u001b[39m\u001b[33m\"\u001b[39m: zarr_format,\n\u001b[32m   1584\u001b[39m }\n\u001b[32m-> \u001b[39m\u001b[32m1586\u001b[39m ds = \u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_cf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_cf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzarr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_default_indexes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_default_indexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1600\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1601\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1602\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1603\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1604\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1605\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1606\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:596\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, create_default_indexes, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    595\u001b[39m overwrite_encoded_chunks = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33moverwrite_encoded_chunks\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m backend_ds = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m ds = _dataset_from_backend_dataset(\n\u001b[32m    603\u001b[39m     backend_ds,\n\u001b[32m    604\u001b[39m     filename_or_obj,\n\u001b[32m   (...)\u001b[39m\u001b[32m    615\u001b[39m     **kwargs,\n\u001b[32m    616\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/zarr.py:1660\u001b[39m, in \u001b[36mZarrBackendEntrypoint.open_dataset\u001b[39m\u001b[34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, zarr_version, zarr_format, store, engine, use_zarr_fill_value_as_mask, cache_members)\u001b[39m\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m store:\n\u001b[32m-> \u001b[39m\u001b[32m1660\u001b[39m     store = \u001b[43mZarrStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_members\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_members\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1675\u001b[39m store_entrypoint = StoreBackendEntrypoint()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/zarr.py:714\u001b[39m, in \u001b[36mZarrStore.open_group\u001b[39m\u001b[34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, align_chunks, zarr_version, zarr_format, use_zarr_fill_value_as_mask, write_empty, cache_members)\u001b[39m\n\u001b[32m    688\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_group\u001b[39m(\n\u001b[32m    690\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    707\u001b[39m     cache_members: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    708\u001b[39m ):\n\u001b[32m    709\u001b[39m     (\n\u001b[32m    710\u001b[39m         zarr_group,\n\u001b[32m    711\u001b[39m         consolidate_on_close,\n\u001b[32m    712\u001b[39m         close_store_on_close,\n\u001b[32m    713\u001b[39m         use_zarr_fill_value_as_mask,\n\u001b[32m--> \u001b[39m\u001b[32m714\u001b[39m     ) = \u001b[43m_get_open_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    728\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    729\u001b[39m         zarr_group,\n\u001b[32m    730\u001b[39m         mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    739\u001b[39m         cache_members=cache_members,\n\u001b[32m    740\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/zarr.py:1864\u001b[39m, in \u001b[36m_get_open_params\u001b[39m\u001b[34m(store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, zarr_version, use_zarr_fill_value_as_mask, zarr_format)\u001b[39m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m consolidated:\n\u001b[32m   1863\u001b[39m     \u001b[38;5;66;03m# TODO: an option to pass the metadata_key keyword\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1864\u001b[39m     zarr_root_group = \u001b[43mzarr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_consolidated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1865\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m consolidated \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1866\u001b[39m     \u001b[38;5;66;03m# same but with more error handling in case no consolidated metadata found\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/api/synchronous.py:231\u001b[39m, in \u001b[36mopen_consolidated\u001b[39m\u001b[34m(use_consolidated, *args, **kwargs)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03mAlias for :func:`open_group` with ``use_consolidated=True``.\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Group(\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_consolidated\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_consolidated\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_consolidated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/core/sync.py:163\u001b[39m, in \u001b[36msync\u001b[39m\u001b[34m(coro, loop, timeout)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/core/sync.py:119\u001b[39m, in \u001b[36m_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/api/asynchronous.py:408\u001b[39m, in \u001b[36mopen_consolidated\u001b[39m\u001b[34m(use_consolidated, *args, **kwargs)\u001b[39m\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    405\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_consolidated\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33mTrue\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in \u001b[39m\u001b[33m'\u001b[39m\u001b[33mopen_consolidated\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. Use \u001b[39m\u001b[33m'\u001b[39m\u001b[33mopen\u001b[39m\u001b[33m'\u001b[39m\u001b[33m with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_consolidated=False\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to bypass consolidated metadata.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m open_group(*args, use_consolidated=use_consolidated, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/api/asynchronous.py:872\u001b[39m, in \u001b[36mopen_group\u001b[39m\u001b[34m(store, mode, cache_attrs, synchronizer, path, chunk_store, storage_options, zarr_version, zarr_format, meta_array, attributes, use_consolidated)\u001b[39m\n\u001b[32m    871\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo group found in store \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m at path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore_path.path\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m872\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m GroupNotFoundError(msg)\n",
      "\u001b[31mGroupNotFoundError\u001b[39m: No group found in store <fsspec.mapping.FSMap object at 0x7dbc7f36f410> at path ''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mGroupNotFoundError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    142\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m xr.open_zarr(mapper, consolidated=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m ds_u = \u001b[43mopen_zarr_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU10M_STORE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m ds_v = open_zarr_s3(V10M_STORE)\n\u001b[32m    147\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ ds_u opened:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(ds_u.data_vars))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mopen_zarr_s3\u001b[39m\u001b[34m(store)\u001b[39m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xr.open_zarr(mapper, consolidated=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_zarr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/zarr.py:1586\u001b[39m, in \u001b[36mopen_zarr\u001b[39m\u001b[34m(store, group, synchronizer, chunks, decode_cf, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, consolidated, overwrite_encoded_chunks, chunk_store, storage_options, decode_timedelta, use_cftime, zarr_version, zarr_format, use_zarr_fill_value_as_mask, chunked_array_type, from_array_kwargs, create_default_indexes, **kwargs)\u001b[39m\n\u001b[32m   1572\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1573\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mopen_zarr() got unexpected keyword arguments \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(kwargs.keys())\n\u001b[32m   1574\u001b[39m     )\n\u001b[32m   1576\u001b[39m backend_kwargs = {\n\u001b[32m   1577\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msynchronizer\u001b[39m\u001b[33m\"\u001b[39m: synchronizer,\n\u001b[32m   1578\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconsolidated\u001b[39m\u001b[33m\"\u001b[39m: consolidated,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1583\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mzarr_format\u001b[39m\u001b[33m\"\u001b[39m: zarr_format,\n\u001b[32m   1584\u001b[39m }\n\u001b[32m-> \u001b[39m\u001b[32m1586\u001b[39m ds = \u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_cf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_cf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzarr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_default_indexes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_default_indexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1600\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1601\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1602\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1603\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1604\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1605\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1606\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:596\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, create_default_indexes, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    584\u001b[39m decoders = _resolve_decoders_kwargs(\n\u001b[32m    585\u001b[39m     decode_cf,\n\u001b[32m    586\u001b[39m     open_backend_dataset_parameters=backend.open_dataset_parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    592\u001b[39m     decode_coords=decode_coords,\n\u001b[32m    593\u001b[39m )\n\u001b[32m    595\u001b[39m overwrite_encoded_chunks = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33moverwrite_encoded_chunks\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m backend_ds = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m ds = _dataset_from_backend_dataset(\n\u001b[32m    603\u001b[39m     backend_ds,\n\u001b[32m    604\u001b[39m     filename_or_obj,\n\u001b[32m   (...)\u001b[39m\u001b[32m    615\u001b[39m     **kwargs,\n\u001b[32m    616\u001b[39m )\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/zarr.py:1660\u001b[39m, in \u001b[36mZarrBackendEntrypoint.open_dataset\u001b[39m\u001b[34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, zarr_version, zarr_format, store, engine, use_zarr_fill_value_as_mask, cache_members)\u001b[39m\n\u001b[32m   1658\u001b[39m filename_or_obj = _normalize_path(filename_or_obj)\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m store:\n\u001b[32m-> \u001b[39m\u001b[32m1660\u001b[39m     store = \u001b[43mZarrStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_members\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_members\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1675\u001b[39m store_entrypoint = StoreBackendEntrypoint()\n\u001b[32m   1676\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/zarr.py:714\u001b[39m, in \u001b[36mZarrStore.open_group\u001b[39m\u001b[34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, align_chunks, zarr_version, zarr_format, use_zarr_fill_value_as_mask, write_empty, cache_members)\u001b[39m\n\u001b[32m    688\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_group\u001b[39m(\n\u001b[32m    690\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    707\u001b[39m     cache_members: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    708\u001b[39m ):\n\u001b[32m    709\u001b[39m     (\n\u001b[32m    710\u001b[39m         zarr_group,\n\u001b[32m    711\u001b[39m         consolidate_on_close,\n\u001b[32m    712\u001b[39m         close_store_on_close,\n\u001b[32m    713\u001b[39m         use_zarr_fill_value_as_mask,\n\u001b[32m--> \u001b[39m\u001b[32m714\u001b[39m     ) = \u001b[43m_get_open_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    728\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    729\u001b[39m         zarr_group,\n\u001b[32m    730\u001b[39m         mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    739\u001b[39m         cache_members=cache_members,\n\u001b[32m    740\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/zarr.py:1902\u001b[39m, in \u001b[36m_get_open_params\u001b[39m\u001b[34m(store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, zarr_version, use_zarr_fill_value_as_mask, zarr_format)\u001b[39m\n\u001b[32m   1898\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _zarr_v3():\n\u001b[32m   1899\u001b[39m         \u001b[38;5;66;03m# we have determined that we don't want to use consolidated metadata\u001b[39;00m\n\u001b[32m   1900\u001b[39m         \u001b[38;5;66;03m# so we set that to False to avoid trying to read it\u001b[39;00m\n\u001b[32m   1901\u001b[39m         open_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_consolidated\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1902\u001b[39m     zarr_group = \u001b[43mzarr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1904\u001b[39m close_store_on_close = zarr_group.store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m store\n\u001b[32m   1906\u001b[39m \u001b[38;5;66;03m# we use this to determine how to handle fill_value\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/api/synchronous.py:540\u001b[39m, in \u001b[36mopen_group\u001b[39m\u001b[34m(store, mode, cache_attrs, synchronizer, path, chunk_store, storage_options, zarr_version, zarr_format, meta_array, attributes, use_consolidated)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_group\u001b[39m(\n\u001b[32m    464\u001b[39m     store: StoreLike | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    465\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    476\u001b[39m     use_consolidated: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    477\u001b[39m ) -> Group:\n\u001b[32m    478\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Open a group using file-mode-like semantics.\u001b[39;00m\n\u001b[32m    479\u001b[39m \n\u001b[32m    480\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    537\u001b[39m \u001b[33;03m        The new group.\u001b[39;00m\n\u001b[32m    538\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    539\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Group(\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m         \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m            \u001b[49m\u001b[43masync_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcache_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m                \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m                \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m                \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m                \u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmeta_array\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmeta_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m                \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m                \u001b[49m\u001b[43muse_consolidated\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_consolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    556\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/core/sync.py:163\u001b[39m, in \u001b[36msync\u001b[39m\u001b[34m(coro, loop, timeout)\u001b[39m\n\u001b[32m    160\u001b[39m return_result = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(finished)).result()\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/core/sync.py:119\u001b[39m, in \u001b[36m_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[33;03mAwait a coroutine and return the result of running it. If awaiting the coroutine raises an\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03mexception, the exception will be returned.\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/api/asynchronous.py:872\u001b[39m, in \u001b[36mopen_group\u001b[39m\u001b[34m(store, mode, cache_attrs, synchronizer, path, chunk_store, storage_options, zarr_version, zarr_format, meta_array, attributes, use_consolidated)\u001b[39m\n\u001b[32m    865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m AsyncGroup.from_store(\n\u001b[32m    866\u001b[39m         store_path,\n\u001b[32m    867\u001b[39m         zarr_format=_zarr_format,\n\u001b[32m    868\u001b[39m         overwrite=overwrite,\n\u001b[32m    869\u001b[39m         attributes=attributes,\n\u001b[32m    870\u001b[39m     )\n\u001b[32m    871\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo group found in store \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m at path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore_path.path\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m872\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m GroupNotFoundError(msg)\n",
      "\u001b[31mGroupNotFoundError\u001b[39m: No group found in store <fsspec.mapping.FSMap object at 0x7dbc7f36f410> at path ''"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5) Define ROI (Soundview) + subset HRRR winds using 2D lat/lon\n",
    "#     Works even when latitude/longitude are (y,x) 2D arrays.\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "print(\"Wind dims:\", wind.dims)\n",
    "print(\"Wind coords:\", list(wind.coords))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (A) Define your ROI in WGS84 (lon/lat)\n",
    "#     Soundview Bronx (approx bbox). You can adjust these later.\n",
    "# ------------------------------------------------------------\n",
    "ROI_MIN_LON = -73.882\n",
    "ROI_MAX_LON = -73.842\n",
    "ROI_MIN_LAT = 40.807\n",
    "ROI_MAX_LAT = 40.836\n",
    "\n",
    "print(\"\\nROI WGS84 bbox:\")\n",
    "print(\"  lon:\", (ROI_MIN_LON, ROI_MAX_LON))\n",
    "print(\"  lat:\", (ROI_MIN_LAT, ROI_MAX_LAT))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (B) Grab 2D lat/lon from dataset\n",
    "# ------------------------------------------------------------\n",
    "if \"latitude\" not in wind.coords or \"longitude\" not in wind.coords:\n",
    "    raise RuntimeError(\"Expected wind coords 'latitude' and 'longitude' but did not find them.\")\n",
    "\n",
    "lat2d = wind[\"latitude\"]\n",
    "lon2d = wind[\"longitude\"]\n",
    "\n",
    "# Ensure they are (y,x)\n",
    "print(\"\\nLatitude shape:\", lat2d.shape, \"Longitude shape:\", lon2d.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (C) Normalize longitudes if stored as 0..360\n",
    "# ------------------------------------------------------------\n",
    "lon_vals = lon2d.values\n",
    "if np.nanmax(lon_vals) > 180:\n",
    "    print(\"Detected 0..360 longitude convention -> converting to -180..180\")\n",
    "    lon2d_fixed = ((lon2d + 180) % 360) - 180\n",
    "else:\n",
    "    lon2d_fixed = lon2d\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (D) Build ROI mask on the 2D grid\n",
    "# ------------------------------------------------------------\n",
    "mask = (\n",
    "    (lat2d >= ROI_MIN_LAT) & (lat2d <= ROI_MAX_LAT) &\n",
    "    (lon2d_fixed >= ROI_MIN_LON) & (lon2d_fixed <= ROI_MAX_LON)\n",
    ")\n",
    "\n",
    "mask_count = int(mask.sum().values) if hasattr(mask.sum().values, \"item\") else int(mask.sum().values)\n",
    "print(\"\\nMask pixels inside ROI:\", mask_count)\n",
    "\n",
    "if mask_count == 0:\n",
    "    # Helpful debugging: print dataset geographic extent\n",
    "    lat_min = float(np.nanmin(lat2d.values))\n",
    "    lat_max = float(np.nanmax(lat2d.values))\n",
    "    lon_min = float(np.nanmin(lon2d_fixed.values))\n",
    "    lon_max = float(np.nanmax(lon2d_fixed.values))\n",
    "    raise RuntimeError(\n",
    "        \"ROI mask returned 0 pixels.\\n\"\n",
    "        f\"Wind lat range: {lat_min:.4f} .. {lat_max:.4f}\\n\"\n",
    "        f\"Wind lon range: {lon_min:.4f} .. {lon_max:.4f}\\n\"\n",
    "        \"Your ROI bbox is outside the dataset coverage OR lon convention mismatch.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (E) Convert mask -> bounding box indices (y_min..y_max, x_min..x_max)\n",
    "# ------------------------------------------------------------\n",
    "yy, xx = np.where(mask.values)\n",
    "\n",
    "y0, y1 = int(yy.min()), int(yy.max())\n",
    "x0, x1 = int(xx.min()), int(xx.max())\n",
    "\n",
    "# Add a small pad so we don't clip tightly\n",
    "PAD = 4\n",
    "y0 = max(0, y0 - PAD)\n",
    "x0 = max(0, x0 - PAD)\n",
    "y1 = min(wind.sizes[\"y\"] - 1, y1 + PAD)\n",
    "x1 = min(wind.sizes[\"x\"] - 1, x1 + PAD)\n",
    "\n",
    "print(\"\\nSubset index window:\")\n",
    "print(\"  y:\", (y0, y1), \"=> height:\", (y1 - y0 + 1))\n",
    "print(\"  x:\", (x0, x1), \"=> width :\", (x1 - x0 + 1))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (F) Subset wind by y/x index window\n",
    "# ------------------------------------------------------------\n",
    "wind_roi = wind.isel(y=slice(y0, y1 + 1), x=slice(x0, x1 + 1))\n",
    "\n",
    "print(\"\\n‚úÖ Wind subset done.\")\n",
    "print(\"Subset dims:\", wind_roi.dims)\n",
    "\n",
    "# Replace wind with ROI subset for the rest of notebook\n",
    "wind = wind_roi\n",
    "\n",
    "# ============================================================\n",
    "# 5.9) FORCE OPEN HRRR u10m/v10m from YEAR=2025 stores (NO STALE DS)\n",
    "# ============================================================\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import s3fs\n",
    "\n",
    "YEAR = 2025\n",
    "\n",
    "# --- These MUST be correct ---\n",
    "# Example OSN_ROOT: \"s3://leap-pangeo-manual/hackathon-2026/\"\n",
    "# (Make sure your OSN_ROOT ends with \"/\")\n",
    "if \"OSN_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"‚ùå OSN_ROOT not defined. It should be like: s3://leap-pangeo-manual/hackathon-2026/\")\n",
    "\n",
    "U10M_STORE = f\"{OSN_ROOT}hrrr/u10m/hrrru10m{YEAR}.zarr\"\n",
    "V10M_STORE = f\"{OSN_ROOT}hrrr/v10m/hrrrv10m{YEAR}.zarr\"\n",
    "\n",
    "print(\"‚úÖ Using YEAR:\", YEAR)\n",
    "print(\"U10M_STORE:\", U10M_STORE)\n",
    "print(\"V10M_STORE:\", V10M_STORE)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) DELETE stale variables if they exist\n",
    "# ------------------------------------------------------------\n",
    "for stale in [\"ds_u\", \"ds_v\", \"ds_u_sub\", \"ds_v_sub\", \"ds_u_roi\", \"ds_v_roi\", \"u\", \"v\", \"wind_fews\"]:\n",
    "    if stale in globals():\n",
    "        del globals()[stale]\n",
    "        print(\"üßπ deleted stale:\", stale)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Open via s3fs (important: DO NOT use gcsfs here)\n",
    "# ------------------------------------------------------------\n",
    "fs_s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "def open_zarr_s3(store: str) -> xr.Dataset:\n",
    "    mapper = fs_s3.get_mapper(store)\n",
    "    try:\n",
    "        return xr.open_zarr(mapper, consolidated=True)\n",
    "    except Exception:\n",
    "        return xr.open_zarr(mapper, consolidated=False)\n",
    "\n",
    "ds_u = open_zarr_s3(U10M_STORE)\n",
    "ds_v = open_zarr_s3(V10M_STORE)\n",
    "\n",
    "print(\"\\n‚úÖ ds_u opened:\", list(ds_u.data_vars))\n",
    "print(\"‚úÖ ds_v opened:\", list(ds_v.data_vars))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Verify time really belongs to 2025\n",
    "# ------------------------------------------------------------\n",
    "t0_u = ds_u[\"time\"].values[0]\n",
    "t1_u = ds_u[\"time\"].values[-1]\n",
    "t0_v = ds_v[\"time\"].values[0]\n",
    "t1_v = ds_v[\"time\"].values[-1]\n",
    "\n",
    "print(\"\\n--- TIME CHECK ---\")\n",
    "print(\"u10 time:\", t0_u, \"->\", t1_u)\n",
    "print(\"v10 time:\", t0_v, \"->\", t1_v)\n",
    "\n",
    "ymin = int(np.min(ds_u[\"time\"].dt.year.values))\n",
    "ymax = int(np.max(ds_u[\"time\"].dt.year.values))\n",
    "print(\"u10 year span:\", ymin, \"->\", ymax)\n",
    "\n",
    "if ymin != YEAR and ymax != YEAR:\n",
    "    raise RuntimeError(\n",
    "        f\"‚ùå ds_u is NOT from {YEAR}.\\n\"\n",
    "        f\"Got year span: {ymin}->{ymax}\\n\"\n",
    "        \"This indicates ds_u is not truly using the 2025 store.\"\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ SUCCESS: u10m/v10m are truly YEAR=2025.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0958ab-a0a6-4b07-8510-425f70760ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6 (FIXED): Force matching u10m/v10m to a specific YEAR\n",
    "# ============================================================\n",
    "\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "YEAR = 2025  # <-- force this year only\n",
    "\n",
    "OSN_ENDPOINT_URL = \"https://nyu1.osn.mghpcc.org\"\n",
    "OSN_BUCKET = \"leap-pangeo-manual\"\n",
    "HACKATHON_PREFIX = \"hackathon-2026/\"\n",
    "\n",
    "U10M_STORE = f\"s3://{OSN_BUCKET}/{HACKATHON_PREFIX}hrrr/u10m/hrrru10m{YEAR}.zarr\"\n",
    "V10M_STORE = f\"s3://{OSN_BUCKET}/{HACKATHON_PREFIX}hrrr/v10m/hrrrv10m{YEAR}.zarr\"\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": OSN_ENDPOINT_URL}, anon=True)\n",
    "\n",
    "def open_zarr_store(store_path):\n",
    "    mapper = fs.get_mapper(store_path)\n",
    "    return xr.open_zarr(mapper, consolidated=False)\n",
    "\n",
    "print(\"‚úÖ Using YEAR:\", YEAR)\n",
    "print(\"U10M_STORE:\", U10M_STORE)\n",
    "print(\"V10M_STORE:\", V10M_STORE)\n",
    "\n",
    "ds_u = open_zarr_store(U10M_STORE)\n",
    "ds_v = open_zarr_store(V10M_STORE)\n",
    "\n",
    "print(\"\\n--- ds_u ---\")\n",
    "print(ds_u)\n",
    "print(\"u vars:\", list(ds_u.data_vars))\n",
    "\n",
    "print(\"\\n--- ds_v ---\")\n",
    "print(ds_v)\n",
    "print(\"v vars:\", list(ds_v.data_vars))\n",
    "\n",
    "print(\"\\n‚úÖ Time ranges:\")\n",
    "print(\"u:\", ds_u.time.values[0], \"->\", ds_u.time.values[-1])\n",
    "print(\"v:\", ds_v.time.values[0], \"->\", ds_v.time.values[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af8a53-6830-4181-9b6f-71232bd67a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) Subset u10/v10 to ROI + Build FEWS wind forcing dataset\n",
    "#    ‚úÖ FORCE using ds_u/ds_v opened for YEAR=2025 (no stale globals)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pyproj import Transformer\n",
    "\n",
    "FEWS_TIME_UNITS = \"minutes since 1970-01-01 00:00:00.0 +0000\"\n",
    "WIND_NODATA = -9999.0\n",
    "TARGET_CRS = \"EPSG:26918\"   # UTM18N (matches your DEM)\n",
    "\n",
    "YEAR = 2025\n",
    "\n",
    "# ---- ROI bbox (WGS84) ----\n",
    "ROI_LON_MIN, ROI_LON_MAX = -73.882, -73.842\n",
    "ROI_LAT_MIN, ROI_LAT_MAX =  40.807,  40.836\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) FORCE pick u/v from ds_u / ds_v ONLY\n",
    "# ------------------------------------------------------------\n",
    "if \"ds_u\" not in globals() or \"ds_v\" not in globals():\n",
    "    raise RuntimeError(\"‚ùå ds_u / ds_v not found. Run the HRRR open-zarr cell first.\")\n",
    "\n",
    "def pick_var(ds: xr.Dataset, preferred=(\"u10\", \"v10\")) -> xr.DataArray:\n",
    "    for vn in preferred:\n",
    "        if vn in ds.data_vars:\n",
    "            return ds[vn]\n",
    "    return ds[list(ds.data_vars)[0]]\n",
    "\n",
    "u = pick_var(ds_u, preferred=(\"u10\",))\n",
    "v = pick_var(ds_v, preferred=(\"v10\",))\n",
    "\n",
    "print(\"‚úÖ picked u:\", u.name, \"| v:\", v.name)\n",
    "print(\"Raw u dims:\", u.dims, \"shape:\", u.shape)\n",
    "print(\"Raw v dims:\", v.dims, \"shape:\", v.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Drop nondim coords that break merges (valid_time/step/etc.)\n",
    "# ------------------------------------------------------------\n",
    "def drop_nondim_coords(da: xr.DataArray) -> xr.DataArray:\n",
    "    drop = [c for c in da.coords if c not in da.dims]\n",
    "    return da.drop_vars(drop, errors=\"ignore\")\n",
    "\n",
    "u = drop_nondim_coords(u)\n",
    "v = drop_nondim_coords(v)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Force dim order to (time, y, x)\n",
    "# ------------------------------------------------------------\n",
    "def to_time_y_x(da: xr.DataArray) -> xr.DataArray:\n",
    "    needed = (\"time\", \"y\", \"x\")\n",
    "    if not set(needed).issubset(set(da.dims)):\n",
    "        raise RuntimeError(f\"Expected dims {needed}, got {da.dims}\")\n",
    "    return da.transpose(\"time\", \"y\", \"x\")\n",
    "\n",
    "u = to_time_y_x(u)\n",
    "v = to_time_y_x(v)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Align time by intersection\n",
    "# ------------------------------------------------------------\n",
    "t_u = np.asarray(u[\"time\"].values)\n",
    "t_v = np.asarray(v[\"time\"].values)\n",
    "t_common = np.intersect1d(t_u, t_v)\n",
    "\n",
    "if t_common.size == 0:\n",
    "    raise RuntimeError(\"‚ùå No overlapping timestamps between u and v after cleanup.\")\n",
    "\n",
    "u = u.sel(time=t_common)\n",
    "v = v.sel(time=t_common)\n",
    "\n",
    "print(\"‚úÖ common time len:\", u.sizes[\"time\"])\n",
    "print(\"‚úÖ common time first/last:\", u.time.values[0], \"->\", u.time.values[-1])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) HARD ASSERT: must be YEAR=2025\n",
    "# ------------------------------------------------------------\n",
    "year_min = int(np.min(u[\"time\"].dt.year.values))\n",
    "year_max = int(np.max(u[\"time\"].dt.year.values))\n",
    "print(f\"‚úÖ Wind time span years: {year_min} -> {year_max}\")\n",
    "\n",
    "if year_max < YEAR or year_min > YEAR:\n",
    "    raise RuntimeError(\n",
    "        f\"‚ùå Wind data is NOT in {YEAR}.\\n\"\n",
    "        f\"Found years: {year_min} -> {year_max}\\n\"\n",
    "        \"This means you are NOT actually using the 2025 HRRR store.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) ROI mask via 2D latitude/longitude\n",
    "# ------------------------------------------------------------\n",
    "lat2d = None\n",
    "lon2d = None\n",
    "\n",
    "# HRRR stores usually have 2D latitude/longitude coords\n",
    "for cand_ds in [ds_u, ds_v]:\n",
    "    if (\"latitude\" in cand_ds.coords) and (\"longitude\" in cand_ds.coords):\n",
    "        lat2d = np.asarray(cand_ds[\"latitude\"].values)\n",
    "        lon2d = np.asarray(cand_ds[\"longitude\"].values)\n",
    "        break\n",
    "\n",
    "if lat2d is None or lon2d is None:\n",
    "    raise RuntimeError(\"‚ùå Could not find latitude/longitude coords in ds_u or ds_v.\")\n",
    "\n",
    "# Convert 0..360 -> -180..180 if needed\n",
    "if np.nanmax(lon2d) > 180:\n",
    "    lon2d = ((lon2d + 180) % 360) - 180\n",
    "\n",
    "inside = (\n",
    "    (lon2d >= ROI_LON_MIN) & (lon2d <= ROI_LON_MAX) &\n",
    "    (lat2d >= ROI_LAT_MIN) & (lat2d <= ROI_LAT_MAX)\n",
    ")\n",
    "\n",
    "n_inside = int(np.sum(inside))\n",
    "print(\"Mask pixels inside ROI:\", n_inside)\n",
    "if n_inside == 0:\n",
    "    raise RuntimeError(\"‚ùå ROI mask selected 0 pixels. Check bbox/coords.\")\n",
    "\n",
    "ys, xs = np.where(inside)\n",
    "y0, y1 = int(ys.min()), int(ys.max())\n",
    "x0, x1 = int(xs.min()), int(xs.max())\n",
    "\n",
    "print(\"Subset index window:\")\n",
    "print(f\"  y: ({y0}, {y1}) -> height: {y1-y0+1}\")\n",
    "print(f\"  x: ({x0}, {x1}) -> width : {x1-x0+1}\")\n",
    "\n",
    "u = u.isel(y=slice(y0, y1 + 1), x=slice(x0, x1 + 1))\n",
    "v = v.isel(y=slice(y0, y1 + 1), x=slice(x0, x1 + 1))\n",
    "\n",
    "lat_roi = lat2d[y0:y1 + 1, x0:x1 + 1]\n",
    "lon_roi = lon2d[y0:y1 + 1, x0:x1 + 1]\n",
    "\n",
    "print(\"‚úÖ u subset shape:\", u.shape, \"| v subset shape:\", v.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Project ROI lat/lon -> UTM x/y centers\n",
    "# ------------------------------------------------------------\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", TARGET_CRS, always_xy=True)\n",
    "x2d, y2d = transformer.transform(lon_roi, lat_roi)\n",
    "\n",
    "x2d = np.asarray(x2d, dtype=\"float64\")\n",
    "y2d = np.asarray(y2d, dtype=\"float64\")\n",
    "\n",
    "x_1d = np.nanmean(x2d, axis=0)\n",
    "y_1d = np.nanmean(y2d, axis=1)\n",
    "\n",
    "# Ensure x/y monotonic increasing\n",
    "if np.any(np.diff(x_1d) < 0):\n",
    "    x_1d = x_1d[::-1]\n",
    "    u = u.isel(x=slice(None, None, -1))\n",
    "    v = v.isel(x=slice(None, None, -1))\n",
    "\n",
    "if np.any(np.diff(y_1d) < 0):\n",
    "    y_1d = y_1d[::-1]\n",
    "    u = u.isel(y=slice(None, None, -1))\n",
    "    v = v.isel(y=slice(None, None, -1))\n",
    "\n",
    "print(\"‚úÖ final x/y sizes:\", len(x_1d), len(y_1d))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Build FEWS dataset (amu/amv) with datetime time axis\n",
    "# ------------------------------------------------------------\n",
    "amu = u.astype(\"float32\").where(np.isfinite(u), WIND_NODATA)\n",
    "amv = v.astype(\"float32\").where(np.isfinite(v), WIND_NODATA)\n",
    "\n",
    "wind_fews = xr.Dataset(\n",
    "    data_vars={\n",
    "        \"amu\": ((\"time\", \"y\", \"x\"), amu.values),\n",
    "        \"amv\": ((\"time\", \"y\", \"x\"), amv.values),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": (\"time\", u[\"time\"].values),   # ‚úÖ keep as datetime64\n",
    "        \"x\": (\"x\", x_1d.astype(\"float64\")),\n",
    "        \"y\": (\"y\", y_1d.astype(\"float64\")),\n",
    "    },\n",
    ")\n",
    "\n",
    "wind_fews[\"amu\"].attrs.update({\"long_name\": \"x_wind\", \"units\": \"m s-1\", \"_FillValue\": WIND_NODATA})\n",
    "wind_fews[\"amv\"].attrs.update({\"long_name\": \"y_wind\", \"units\": \"m s-1\", \"_FillValue\": WIND_NODATA})\n",
    "wind_fews.attrs[\"crs\"] = TARGET_CRS\n",
    "\n",
    "print(\"\\n‚úÖ FEWS wind dataset built:\")\n",
    "print(wind_fews)\n",
    "print(\"Time first/last:\", wind_fews.time.values[0], \"->\", wind_fews.time.values[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497d11b-7db7-428a-a19a-562fabd9bbaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7) Write FEWS netamuamvfile.nc + upload to SCRATCH_BUCKET via gcsfs\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import gcsfs\n",
    "\n",
    "# ---- Use your preferred scratch bucket location\n",
    "SCRATCH_BUCKET = os.environ.get(\"SCRATCH_BUCKET\", \"gs://leap-scratch/renriviera\")\n",
    "print(\"‚úÖ Using SCRATCH_BUCKET:\", SCRATCH_BUCKET)\n",
    "\n",
    "# ---- Where inside scratch you want the file\n",
    "# We'll keep your OUT_PREFIX structure but rooted at SCRATCH_BUCKET\n",
    "# Example final path:\n",
    "# gs://leap-scratch/renriviera/sfincs_soundview_preproc/forcing/wind/...\n",
    "out_prefix = f\"{OUT_PREFIX}/forcing/wind\"\n",
    "out_gcs = f\"{out_prefix}/sfincs_netamuamv_hrrr_u10v10_soundview_2025.nc\"\n",
    "\n",
    "# If OUT_PREFIX is already absolute gs://..., override to use SCRATCH_BUCKET explicitly:\n",
    "if out_gcs.startswith(\"gs://\"):\n",
    "    # Make path relative after your scratch root if needed\n",
    "    # If your OUT_PREFIX already starts with SCRATCH_BUCKET, keep it\n",
    "    if not out_gcs.startswith(SCRATCH_BUCKET):\n",
    "        # fallback: store under SCRATCH_BUCKET/forcing/wind/\n",
    "        out_gcs = f\"{SCRATCH_BUCKET}/forcing/wind/sfincs_netamuamv_hrrr_u10v10_soundview_2025.nc\"\n",
    "\n",
    "print(\"üìå Target scratch path:\", out_gcs)\n",
    "\n",
    "# ---- Write locally\n",
    "local_dir = Path(tempfile.mkdtemp(prefix=\"sfincs_wind_fews_\"))\n",
    "out_local = local_dir / \"netamuamvfile.nc\"\n",
    "\n",
    "print(\"Writing local:\", out_local)\n",
    "\n",
    "# IMPORTANT: remove _FillValue from attrs if you also set it in encoding\n",
    "for var in [\"amu\", \"amv\"]:\n",
    "    if \"_FillValue\" in wind_fews[var].attrs:\n",
    "        wind_fews[var].attrs.pop(\"_FillValue\", None)\n",
    "\n",
    "# NetCDF encoding\n",
    "WIND_NODATA = float(WIND_NODATA)\n",
    "encoding = {\n",
    "    \"amu\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"_FillValue\": WIND_NODATA},\n",
    "    \"amv\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"_FillValue\": WIND_NODATA},\n",
    "}\n",
    "\n",
    "f = wind_fews.to_netcdf(out_local, encoding=encoding)\n",
    "print(\"@@@@@@\", f)\n",
    "print(\"‚úÖ Local netcdf written:\", out_local)\n",
    "\n",
    "# ---- Upload with gcsfs (no gcloud/gsutil)\n",
    "fs_gcs = gcsfs.GCSFileSystem(token=\"cloud\")\n",
    "\n",
    "# Convert gs://bucket/path -> bucket/path for gcsfs\n",
    "assert out_gcs.startswith(\"gs://\")\n",
    "gcs_path_no_scheme = out_gcs.replace(\"gs://\", \"\", 1)\n",
    "\n",
    "print(\"Uploading via gcsfs ->\", out_gcs)\n",
    "fs_gcs.put(str(out_local), gcs_path_no_scheme)\n",
    "print(\"‚úÖ Uploaded netamuamvfile to:\", out_gcs)\n",
    "\n",
    "# ---- Quick existence check\n",
    "print(\"Exists on GCS:\", fs_gcs.exists(gcs_path_no_scheme))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff45f7-f181-46c7-882b-6a2bf41d9765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8) Validate uploaded FEWS netamuamvfile.nc for HydroMT-SFINCS\n",
    "#    Checks: existence, vars, dims, dtype, nodata, monotonic time,\n",
    "#            CRS attrs, finite values, and basic range sanity.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "\n",
    "SCRATCH_BUCKET = os.environ.get(\"SCRATCH_BUCKET\", \"gs://leap-scratch/renriviera\")\n",
    "print(\"‚úÖ Using SCRATCH_BUCKET:\", SCRATCH_BUCKET)\n",
    "\n",
    "# ---- Point to your uploaded file\n",
    "# If you already have out_gcs from the previous cell, this will use it.\n",
    "# Otherwise set it explicitly here:\n",
    "try:\n",
    "    OUT_NETCDF = out_gcs\n",
    "except NameError:\n",
    "    OUT_NETCDF = f\"{SCRATCH_BUCKET}/forcing/wind/sfincs_netamuamv_hrrr_u10v10_soundview_2025.nc\"\n",
    "\n",
    "print(\"üìå Validating:\", OUT_NETCDF)\n",
    "\n",
    "# ---- Existence check\n",
    "fs = gcsfs.GCSFileSystem(token=\"cloud\")\n",
    "gcs_path_no_scheme = OUT_NETCDF.replace(\"gs://\", \"\", 1)\n",
    "if not fs.exists(gcs_path_no_scheme):\n",
    "    raise FileNotFoundError(f\"‚ùå Not found on GCS: {OUT_NETCDF}\")\n",
    "\n",
    "print(\"‚úÖ File exists on GCS\")\n",
    "\n",
    "# ---- Open (gcsfs -> fsspec) without downloading\n",
    "# (Works well for NetCDF4; if it fails, we fallback to caching locally.)\n",
    "try:\n",
    "    ds = xr.open_dataset(OUT_NETCDF, engine=\"netcdf4\")\n",
    "    print(\"‚úÖ Opened remotely with netcdf4 engine\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Remote open failed, caching locally. Reason:\", type(e).__name__, \"-\", str(e)[:200])\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    local_dir = Path(tempfile.mkdtemp(prefix=\"sfincs_wind_validate_\"))\n",
    "    local_nc = local_dir / \"netamuamvfile.nc\"\n",
    "    fs.get(gcs_path_no_scheme, str(local_nc))\n",
    "    print(\"‚úÖ Downloaded to:\", local_nc)\n",
    "    ds = xr.open_dataset(local_nc, engine=\"netcdf4\")\n",
    "    print(\"‚úÖ Opened locally with netcdf4 engine\")\n",
    "\n",
    "print(\"\\n--- Dataset summary ---\")\n",
    "print(ds)\n",
    "\n",
    "# ============================================================\n",
    "# Required structure for SFINCS FEWS wind forcing\n",
    "# ============================================================\n",
    "\n",
    "REQUIRED_VARS = [\"amu\", \"amv\"]\n",
    "REQUIRED_DIMS = (\"time\", \"y\", \"x\")\n",
    "EXPECTED_FILL = -9999.0\n",
    "\n",
    "# ---- 1) Required variables exist\n",
    "missing_vars = [v for v in REQUIRED_VARS if v not in ds.data_vars]\n",
    "if missing_vars:\n",
    "    raise AssertionError(f\"‚ùå Missing required vars: {missing_vars}\")\n",
    "print(\"‚úÖ Required vars present:\", REQUIRED_VARS)\n",
    "\n",
    "# ---- 2) Each variable has dims (time,y,x)\n",
    "for v in REQUIRED_VARS:\n",
    "    if tuple(ds[v].dims) != REQUIRED_DIMS:\n",
    "        raise AssertionError(f\"‚ùå {v} dims {ds[v].dims} != {REQUIRED_DIMS}\")\n",
    "print(\"‚úÖ Variable dimensions are correct:\", REQUIRED_DIMS)\n",
    "\n",
    "# ---- 3) Dtypes are numeric + float-ish\n",
    "for v in REQUIRED_VARS:\n",
    "    if not np.issubdtype(ds[v].dtype, np.floating):\n",
    "        raise AssertionError(f\"‚ùå {v} dtype {ds[v].dtype} is not float\")\n",
    "print(\"‚úÖ Variable dtypes are float\")\n",
    "\n",
    "# ---- 4) Time coordinate validity\n",
    "if \"time\" not in ds.coords:\n",
    "    raise AssertionError(\"‚ùå Missing time coordinate\")\n",
    "if ds[\"time\"].size < 2:\n",
    "    raise AssertionError(\"‚ùå time coord too short\")\n",
    "if not np.issubdtype(ds[\"time\"].dtype, np.datetime64):\n",
    "    raise AssertionError(f\"‚ùå time dtype should be datetime64, got {ds['time'].dtype}\")\n",
    "\n",
    "t = ds[\"time\"].values\n",
    "if not np.all(np.diff(t).astype(\"timedelta64[s]\") > np.timedelta64(0, \"s\")):\n",
    "    raise AssertionError(\"‚ùå time is not strictly increasing\")\n",
    "print(\"‚úÖ time is datetime64 and strictly increasing\")\n",
    "\n",
    "# ---- 5) Spatial coordinates exist and are 1D\n",
    "for coord in [\"x\", \"y\"]:\n",
    "    if coord not in ds.coords:\n",
    "        raise AssertionError(f\"‚ùå Missing coord: {coord}\")\n",
    "    if ds[coord].ndim != 1:\n",
    "        raise AssertionError(f\"‚ùå {coord} must be 1D, got ndim={ds[coord].ndim}\")\n",
    "print(\"‚úÖ x/y are present and 1D\")\n",
    "\n",
    "# ---- 6) FillValue / missing data check\n",
    "def get_fillvalue(da):\n",
    "    # Prefer encoding _FillValue, fallback to attrs\n",
    "    fv = da.encoding.get(\"_FillValue\", None)\n",
    "    if fv is None:\n",
    "        fv = da.attrs.get(\"_FillValue\", None)\n",
    "    return fv\n",
    "\n",
    "for v in REQUIRED_VARS:\n",
    "    fv = get_fillvalue(ds[v])\n",
    "    if fv is None:\n",
    "        print(f\"‚ö†Ô∏è {v}: no _FillValue found in encoding/attrs (not always fatal)\")\n",
    "    else:\n",
    "        if not np.isclose(float(fv), float(EXPECTED_FILL)):\n",
    "            raise AssertionError(f\"‚ùå {v}: _FillValue={fv} != expected {EXPECTED_FILL}\")\n",
    "        print(f\"‚úÖ {v}: _FillValue OK ({fv})\")\n",
    "\n",
    "# ---- 7) Check for NaNs (should typically be filled, not NaN)\n",
    "for v in REQUIRED_VARS:\n",
    "    # sample a small slice to avoid loading everything\n",
    "    sample = ds[v].isel(time=slice(0, min(48, ds.dims[\"time\"]))).values\n",
    "    if np.isnan(sample).any():\n",
    "        raise AssertionError(f\"‚ùå {v}: contains NaNs (should be filled to nodata={EXPECTED_FILL})\")\n",
    "print(\"‚úÖ No NaNs detected in early time sample\")\n",
    "\n",
    "# ---- 8) Basic range sanity (wind in m/s, very broad allowed)\n",
    "# HRRR winds can spike but this catches unit mistakes like km/h or knots\n",
    "for v in REQUIRED_VARS:\n",
    "    sample = ds[v].isel(time=slice(0, min(168, ds.dims[\"time\"]))).values  # 1 week\n",
    "    # ignore fillvalues\n",
    "    sample = sample[np.isfinite(sample)]\n",
    "    sample = sample[sample != EXPECTED_FILL]\n",
    "    if sample.size == 0:\n",
    "        raise AssertionError(f\"‚ùå {v}: no valid data found after removing fillvalues\")\n",
    "    p99 = float(np.quantile(sample, 0.99))\n",
    "    p01 = float(np.quantile(sample, 0.01))\n",
    "    print(f\"Sanity {v}: p01={p01:.2f}, p99={p99:.2f} (m/s)\")\n",
    "    if abs(p99) > 80 or abs(p01) > 80:\n",
    "        raise AssertionError(f\"‚ùå {v}: suspicious wind magnitude (>80 m/s). Units wrong?\")\n",
    "print(\"‚úÖ Wind magnitude sanity checks passed\")\n",
    "\n",
    "# ---- 9) Optional CRS attribute (nice to have)\n",
    "crs_attr = ds.attrs.get(\"crs\", None)\n",
    "if crs_attr is None:\n",
    "    print(\"‚ö†Ô∏è Dataset attribute 'crs' missing (not always fatal).\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset CRS attr:\", crs_attr)\n",
    "\n",
    "# ---- 10) Final verdict\n",
    "print(\"\\n‚úÖ FEWS netamuamvfile.nc looks HydroMT-SFINCS compatible.\")\n",
    "print(\"   - Vars: amu/amv\")\n",
    "print(\"   - Dims: (time,y,x)\")\n",
    "print(\"   - time is monotonic datetime64\")\n",
    "print(\"   - x/y are 1D coords\")\n",
    "print(\"   - FillValue handled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ede13-ba75-4f37-969c-cbccb2bf5051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
